"""
æ³¨æ„åŠ›æ±‡èšï¼šNadaraya-Watson æ ¸å›å½’
    ä¸ŠèŠ‚ä»‹ç»äº†æ¡†æ¶ä¸‹çš„æ³¨æ„åŠ›æœºåˆ¶çš„ä¸»è¦æˆåˆ†ï¼š
        æŸ¥è¯¢ï¼ˆè‡ªä¸»æç¤ºï¼‰å’Œé”®ï¼ˆéè‡ªä¸»æç¤ºï¼‰ä¹‹é—´çš„äº¤äº’å½¢æˆäº†æ³¨æ„åŠ›æ±‡èšï¼› 
        æ³¨æ„åŠ›æ±‡èšæœ‰é€‰æ‹©åœ°èšåˆäº†å€¼ï¼ˆæ„Ÿå®˜è¾“å…¥ï¼‰ä»¥ç”Ÿæˆæœ€ç»ˆçš„è¾“å‡ºã€‚ 

    æœ¬èŠ‚å°†ä»‹ç»æ³¨æ„åŠ›æ±‡èšçš„æ›´å¤šç»†èŠ‚ï¼Œ ä»¥ä¾¿ä»å®è§‚ä¸Šäº†è§£æ³¨æ„åŠ›æœºåˆ¶åœ¨å®è·µä¸­çš„è¿ä½œæ–¹å¼ã€‚ 
        å…·ä½“æ¥è¯´ï¼Œ1964å¹´æå‡ºçš„Nadaraya-Watsonæ ¸å›å½’æ¨¡å‹ æ˜¯ä¸€ä¸ªç®€å•ä½†å®Œæ•´çš„ä¾‹å­ï¼Œå¯ä»¥ç”¨äºæ¼”ç¤ºå…·æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„æœºå™¨å­¦ä¹ ã€‚
"""
import torch
from torch import nn
from d2l import torch as d2l
"""
        [ç”Ÿæˆæ•°æ®é›†]
        ç®€å•èµ·è§ï¼Œè€ƒè™‘ä¸‹é¢è¿™ä¸ªå›å½’é—®é¢˜ï¼š ç»™å®šçš„æˆå¯¹çš„â€œè¾“å…¥ï¼è¾“å‡ºâ€æ•°æ®é›†  {(ğ‘¥1,ğ‘¦1),â€¦,(ğ‘¥ğ‘›,ğ‘¦ğ‘›)}ï¼Œ å¦‚ä½•å­¦ä¹  ğ‘“ æ¥é¢„æµ‹ä»»æ„æ–°è¾“å…¥ ğ‘¥ çš„è¾“å‡º ğ‘¦Ì‚ =ğ‘“(ğ‘¥) ï¼Ÿ

        æ ¹æ®ä¸‹é¢çš„éçº¿æ€§å‡½æ•°ç”Ÿæˆä¸€ä¸ªäººå·¥æ•°æ®é›†ï¼Œ å…¶ä¸­åŠ å…¥çš„å™ªå£°é¡¹ä¸º ğœ– ï¼š
                                                        ğ‘¦ğ‘–=2sin(ğ‘¥ğ‘–)+ğ‘¥ğ‘–^0.8+ğœ–,
"""
n_train = 50  # è®­ç»ƒæ ·æœ¬æ•°
x_train, _ = torch.sort(torch.rand(n_train) * 5)   # æ’åºåçš„è®­ç»ƒæ ·æœ¬

def f(x):
    return 2 * torch.sin(x) + x**0.8

y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # è®­ç»ƒæ ·æœ¬çš„è¾“å‡º
x_test = torch.arange(0, 5, 0.1)  # æµ‹è¯•æ ·æœ¬
y_truth = f(x_test)  # æµ‹è¯•æ ·æœ¬çš„çœŸå®è¾“å‡º
n_test = len(x_test)  # æµ‹è¯•æ ·æœ¬æ•°
print(n_test)

"""
        ä¸‹é¢çš„å‡½æ•°å°†ç»˜åˆ¶æ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬ï¼ˆæ ·æœ¬ç”±åœ†åœˆè¡¨ç¤ºï¼‰ï¼Œ ä¸å¸¦å™ªå£°é¡¹çš„çœŸå®æ•°æ®ç”Ÿæˆå‡½æ•° ğ‘“ ï¼ˆæ ‡è®°ä¸ºâ€œTruthâ€ï¼‰ï¼Œ ä»¥åŠå­¦ä¹ å¾—åˆ°çš„é¢„æµ‹å‡½æ•°ï¼ˆæ ‡è®°ä¸ºâ€œPredâ€ï¼‰ã€‚
"""
def plot_kernel_reg(y_hat):
    d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'Pred'], xlim=[0, 5], ylim=[-1, 5])
    d2l.plt.plot(x_train, y_train, 'o', alpha=0.5);

"""
        å¹³å‡æ±‡èš
            å…ˆä½¿ç”¨æœ€ç®€å•çš„ä¼°è®¡å™¨æ¥è§£å†³å›å½’é—®é¢˜ã€‚ åŸºäºå¹³å‡æ±‡èšæ¥è®¡ç®—æ‰€æœ‰è®­ç»ƒæ ·æœ¬è¾“å‡ºå€¼çš„å¹³å‡å€¼ï¼š

                               n
                    ğ‘“(ğ‘¥)=1/ğ‘› * âˆ‘ğ‘¦ğ‘–, (1)
                              i=1

        å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿™ä¸ªä¼°è®¡å™¨ç¡®å®ä¸å¤Ÿèªæ˜ã€‚ çœŸå®å‡½æ•° ğ‘“ ï¼ˆâ€œTruthâ€ï¼‰å’Œé¢„æµ‹å‡½æ•°ï¼ˆâ€œPredâ€ï¼‰ç›¸å·®å¾ˆå¤§ã€‚
"""
y_hat = torch.repeat_interleave(y_train.mean(), n_test)
plot_kernel_reg(y_hat)

"""
        [éå‚æ•°æ³¨æ„åŠ›æ±‡èš]

        æ˜¾ç„¶ï¼Œå¹³å‡æ±‡èšå¿½ç•¥äº†è¾“å…¥ ğ‘¥ğ‘–ã€‚ äºæ˜¯Nadarayaå’Œ Watsonæå‡ºäº†ä¸€ä¸ªæ›´å¥½çš„æƒ³æ³•ï¼Œ æ ¹æ®è¾“å…¥çš„ä½ç½®å¯¹è¾“å‡º ğ‘¦ğ‘– è¿›è¡ŒåŠ æƒï¼š

                 n         n
            ğ‘“(ğ‘¥)=âˆ‘(ğ¾(ğ‘¥âˆ’ğ‘¥ğ‘–)/âˆ‘ğ¾(ğ‘¥âˆ’ğ‘¥ğ‘—))ğ‘¦ğ‘–, (2)
                i=1       j=1

            å…¶ä¸­ ğ¾ æ˜¯æ ¸ï¼ˆkernelï¼‰ã€‚ 
        å…¬å¼æ‰€æè¿°çš„ä¼°è®¡å™¨è¢«ç§°ä¸º Nadaraya-Watsonæ ¸å›å½’ï¼ˆNadaraya-Watson kernel regressionï¼‰ã€‚ 
        è¿™é‡Œä¸ä¼šæ·±å…¥è®¨è®ºæ ¸å‡½æ•°çš„ç»†èŠ‚ï¼Œä½†å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¯ä»¥ä»./attention_cues.pyä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æ¡†æ¶çš„è§’åº¦é‡å†™nadaraya-watsonï¼Œ 
        æˆä¸ºä¸€ä¸ªæ›´åŠ é€šç”¨çš„æ³¨æ„åŠ›æ±‡èšï¼ˆattention poolingï¼‰å…¬å¼ï¼š

                 n
            ğ‘“(ğ‘¥)=âˆ‘ğ›¼(ğ‘¥,ğ‘¥ğ‘–)ğ‘¦ğ‘–, (3)
                i=1

            å…¶ä¸­ ğ‘¥ æ˜¯æŸ¥è¯¢ï¼Œ (ğ‘¥ğ‘–,ğ‘¦ğ‘–) æ˜¯é”®å€¼å¯¹ã€‚ 
        æ¯”è¾ƒ (1) å’Œ (3)ï¼Œ 
            å‰è€…æ³¨æ„åŠ›æ±‡èšæ˜¯ ğ‘¦ğ‘– çš„åŠ æƒå¹³å‡ã€‚ 
            åè€…å°†æŸ¥è¯¢ ğ‘¥ å’Œé”® ğ‘¥ğ‘– ä¹‹é—´çš„å…³ç³»å»ºæ¨¡ä¸º æ³¨æ„åŠ›æƒé‡ï¼ˆattention weightï¼‰ ğ›¼(ğ‘¥,ğ‘¥ğ‘–)ï¼Œè¿™ä¸ªæƒé‡å°†è¢«åˆ†é…ç»™æ¯ä¸€ä¸ªå¯¹åº”å€¼ ğ‘¦ğ‘–ã€‚ 
            å¯¹äºä»»ä½•æŸ¥è¯¢ï¼Œæ¨¡å‹åœ¨æ‰€æœ‰é”®å€¼å¯¹æ³¨æ„åŠ›æƒé‡éƒ½æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒï¼š å®ƒä»¬æ˜¯éè´Ÿçš„ï¼Œå¹¶ä¸”æ€»å’Œä¸º1ã€‚

        ä¸ºäº†æ›´å¥½åœ°ç†è§£æ³¨æ„åŠ›æ±‡èšï¼Œ ä¸‹é¢è€ƒè™‘ä¸€ä¸ªé«˜æ–¯æ ¸ï¼ˆGaussian kernelï¼‰ï¼Œå…¶å®šä¹‰ä¸ºï¼š

            ğ¾(ğ‘¢)=1/âˆš2ğœ‹ * exp(âˆ’ğ‘¢^2/2).
         

        å°†é«˜æ–¯æ ¸ä»£å…¥ (2) å’Œ (3) å¯ä»¥å¾—åˆ°ï¼š

                 n
            ğ‘“(ğ‘¥)=âˆ‘ğ›¼(ğ‘¥,ğ‘¥ğ‘–)ğ‘¦ğ‘–
                i=1
                 n                     n
                =âˆ‘(exp(âˆ’1/2 * (ğ‘¥âˆ’ğ‘¥ğ‘–)^2)/âˆ‘exp(âˆ’1/2 * (ğ‘¥âˆ’ğ‘¥ğ‘—)^2)) * ğ‘¦ğ‘–
                i=1                   j=1
                 n
                =âˆ‘softmax(âˆ’1/2 * (ğ‘¥âˆ’ğ‘¥ğ‘–)^2)ğ‘¦ğ‘–. (4)
                i=1
         

            åœ¨ (4) ä¸­ï¼Œ å¦‚æœä¸€ä¸ªé”® ğ‘¥ğ‘– è¶Šæ˜¯æ¥è¿‘ç»™å®šçš„æŸ¥è¯¢ ğ‘¥ ï¼Œé‚£ä¹ˆåˆ†é…ç»™è¿™ä¸ªé”®å¯¹åº”å€¼ ğ‘¦ğ‘–çš„æ³¨æ„åŠ›æƒé‡å°±ä¼šè¶Šå¤§ï¼Œä¹Ÿå°±â€œè·å¾—äº†æ›´å¤šçš„æ³¨æ„åŠ›â€ã€‚

        å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒNadaraya-Watsonæ ¸å›å½’æ˜¯ä¸€ä¸ªéå‚æ•°æ¨¡å‹ã€‚ 
        å› æ­¤ï¼Œ(4) æ˜¯ éå‚æ•°çš„æ³¨æ„åŠ›æ±‡èšï¼ˆnonparametric attention poolingï¼‰æ¨¡å‹ã€‚ 
        æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åŸºäºè¿™ä¸ªéå‚æ•°çš„æ³¨æ„åŠ›æ±‡èšæ¨¡å‹æ¥ç»˜åˆ¶é¢„æµ‹ç»“æœã€‚ 
        ä»ç»˜åˆ¶çš„ç»“æœä¼šå‘ç°æ–°çš„æ¨¡å‹é¢„æµ‹çº¿æ˜¯å¹³æ»‘çš„ï¼Œå¹¶ä¸”æ¯”å¹³å‡æ±‡èšçš„é¢„æµ‹æ›´æ¥è¿‘çœŸå®ã€‚
"""
# X_repeatçš„å½¢çŠ¶:(n_test,n_train),
# æ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„æµ‹è¯•è¾“å…¥ï¼ˆä¾‹å¦‚ï¼šåŒæ ·çš„æŸ¥è¯¢ï¼‰
X_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))
# x_trainåŒ…å«ç€é”®ã€‚attention_weightsçš„å½¢çŠ¶ï¼š(n_test,n_train),
# æ¯ä¸€è¡Œéƒ½åŒ…å«ç€è¦åœ¨ç»™å®šçš„æ¯ä¸ªæŸ¥è¯¢çš„å€¼ï¼ˆy_trainï¼‰ä¹‹é—´åˆ†é…çš„æ³¨æ„åŠ›æƒé‡
attention_weights = nn.functional.softmax(-(X_repeat - x_train)**2 / 2, dim=1)
# y_hatçš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å€¼çš„åŠ æƒå¹³å‡å€¼ï¼Œå…¶ä¸­çš„æƒé‡æ˜¯æ³¨æ„åŠ›æƒé‡
y_hat = torch.matmul(attention_weights, y_train)
plot_kernel_reg(y_hat)

"""
        ç°åœ¨æ¥è§‚å¯Ÿæ³¨æ„åŠ›çš„æƒé‡ã€‚ 
        è¿™é‡Œæµ‹è¯•æ•°æ®çš„è¾“å…¥ç›¸å½“äºæŸ¥è¯¢ï¼Œè€Œè®­ç»ƒæ•°æ®çš„è¾“å…¥ç›¸å½“äºé”®ã€‚ 
        å› ä¸ºä¸¤ä¸ªè¾“å…¥éƒ½æ˜¯ç»è¿‡æ’åºçš„ï¼Œå› æ­¤ç”±è§‚å¯Ÿå¯çŸ¥â€œæŸ¥è¯¢-é”®â€å¯¹è¶Šæ¥è¿‘ï¼Œ æ³¨æ„åŠ›æ±‡èšçš„[æ³¨æ„åŠ›æƒé‡]å°±è¶Šé«˜ã€‚
"""
d2l.show_heatmaps(attention_weights.unsqueeze(0).unsqueeze(0),
                  xlabel='Sorted training inputs',
                  ylabel='Sorted testing inputs')

"""
        [å¸¦å‚æ•°æ³¨æ„åŠ›æ±‡èš]

        éå‚æ•°çš„Nadaraya-Watsonæ ¸å›å½’å…·æœ‰ä¸€è‡´æ€§ï¼ˆconsistencyï¼‰çš„ä¼˜ç‚¹ï¼š å¦‚æœæœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæ­¤æ¨¡å‹ä¼šæ”¶æ•›åˆ°æœ€ä¼˜ç»“æœã€‚ 
        å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜æ˜¯å¯ä»¥è½»æ¾åœ°å°†å¯å­¦ä¹ çš„å‚æ•°é›†æˆåˆ°æ³¨æ„åŠ›æ±‡èšä¸­ã€‚

            ä¾‹å¦‚ï¼Œä¸ (4) ç•¥æœ‰ä¸åŒï¼Œ åœ¨ä¸‹é¢çš„æŸ¥è¯¢ ğ‘¥ å’Œé”® ğ‘¥ğ‘– ä¹‹é—´çš„è·ç¦»ä¹˜ä»¥å¯å­¦ä¹ å‚æ•° ğ‘¤ ï¼š

                     n
                ğ‘“(ğ‘¥)=âˆ‘ğ›¼(ğ‘¥,ğ‘¥ğ‘–)ğ‘¦ğ‘–
                    i=1
                     n                          n
                    =âˆ‘(exp(âˆ’1/2 * ((ğ‘¥âˆ’ğ‘¥ğ‘–)ğ‘¤ )^2)/âˆ‘exp(âˆ’1/2 * ((ğ‘¥âˆ’ğ‘¥ğ‘—)ğ‘¤ )^2)) * ğ‘¦ğ‘–
                    i=1                        j=1
                     n
                    =âˆ‘softmax(âˆ’1/2 * ((ğ‘¥âˆ’ğ‘¥ğ‘–)ğ‘¤ )^2)ğ‘¦ğ‘–. (5)
                    i=1

        æœ¬èŠ‚çš„ä½™ä¸‹éƒ¨åˆ†å°†é€šè¿‡è®­ç»ƒè¿™ä¸ªæ¨¡å‹ (5) æ¥å­¦ä¹ æ³¨æ„åŠ›æ±‡èšçš„å‚æ•°ã€‚

        æ‰¹é‡çŸ©é˜µä¹˜æ³•

        ä¸ºäº†æ›´æœ‰æ•ˆåœ°è®¡ç®—å°æ‰¹é‡æ•°æ®çš„æ³¨æ„åŠ›ï¼Œ æˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ·±åº¦å­¦ä¹ å¼€å‘æ¡†æ¶ä¸­æä¾›çš„æ‰¹é‡çŸ©é˜µä¹˜æ³•ã€‚

        å‡è®¾ç¬¬ä¸€ä¸ªå°æ‰¹é‡æ•°æ®åŒ…å« ğ‘› ä¸ªçŸ©é˜µ ğ—1,â€¦,ğ—ğ‘›ï¼Œ å½¢çŠ¶ä¸º ğ‘Ã—ğ‘ï¼Œ ç¬¬äºŒä¸ªå°æ‰¹é‡åŒ…å« ğ‘› ä¸ªçŸ©é˜µ ğ˜1,â€¦,ğ˜ğ‘›ï¼Œ å½¢çŠ¶ä¸º ğ‘Ã—ğ‘ã€‚ 
        å®ƒä»¬çš„æ‰¹é‡çŸ©é˜µä¹˜æ³•å¾—åˆ° ğ‘› ä¸ªçŸ©é˜µ  ğ—1ğ˜1,â€¦,ğ—ğ‘›ğ˜ğ‘›ï¼Œ å½¢çŠ¶ä¸º ğ‘Ã—ğ‘ã€‚ 
        å› æ­¤ï¼Œ[å‡å®šä¸¤ä¸ªå¼ é‡çš„å½¢çŠ¶åˆ†åˆ«æ˜¯ (ğ‘›,ğ‘,ğ‘) å’Œ (ğ‘›,ğ‘,ğ‘)ï¼Œ å®ƒä»¬çš„æ‰¹é‡çŸ©é˜µä¹˜æ³•è¾“å‡ºçš„å½¢çŠ¶ä¸º (ğ‘›,ğ‘,ğ‘)]ã€‚
"""
X = torch.ones((2, 1, 4))
Y = torch.ones((2, 4, 6))
torch.bmm(X, Y).shape

"""
        åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„èƒŒæ™¯ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥[ä½¿ç”¨å°æ‰¹é‡çŸ©é˜µä¹˜æ³•æ¥è®¡ç®—å°æ‰¹é‡æ•°æ®ä¸­çš„åŠ æƒå¹³å‡å€¼]ã€‚
"""
weights = torch.ones((2, 10)) * 0.1
values = torch.arange(20.0).reshape((2, 10))
torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1))

"""
        å®šä¹‰æ¨¡å‹
        åŸºäº (4) ä¸­çš„ [å¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èš]ï¼Œä½¿ç”¨å°æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼Œ å®šä¹‰Nadaraya-Watsonæ ¸å›å½’çš„å¸¦å‚æ•°ç‰ˆæœ¬ä¸ºï¼š
"""
class NWKernelRegression(nn.Module):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.w = nn.Parameter(torch.rand((1,), requires_grad=True))

    def forward(self, queries, keys, values):
        # querieså’Œattention_weightsçš„å½¢çŠ¶ä¸º(æŸ¥è¯¢ä¸ªæ•°ï¼Œâ€œé”®ï¼å€¼â€å¯¹ä¸ªæ•°)
        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))
        self.attention_weights = nn.functional.softmax(-((queries - keys) * self.w)**2 / 2, dim=1)
        # valuesçš„å½¢çŠ¶ä¸º(æŸ¥è¯¢ä¸ªæ•°ï¼Œâ€œé”®ï¼å€¼â€å¯¹ä¸ªæ•°)
        return torch.bmm(self.attention_weights.unsqueeze(1),
                         values.unsqueeze(-1)).reshape(-1)

"""
    è®­ç»ƒ
    æ¥ä¸‹æ¥ï¼Œ[å°†è®­ç»ƒæ•°æ®é›†å˜æ¢ä¸ºé”®å’Œå€¼]ç”¨äºè®­ç»ƒæ³¨æ„åŠ›æ¨¡å‹ã€‚ 
    åœ¨å¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èšæ¨¡å‹ä¸­ï¼Œ ä»»ä½•ä¸€ä¸ªè®­ç»ƒæ ·æœ¬çš„è¾“å…¥éƒ½ä¼šå’Œé™¤è‡ªå·±ä»¥å¤–çš„æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„â€œé”®ï¼å€¼â€å¯¹è¿›è¡Œè®¡ç®—ï¼Œ ä»è€Œå¾—åˆ°å…¶å¯¹åº”çš„é¢„æµ‹è¾“å‡ºã€‚
"""
# X_tileçš„å½¢çŠ¶:(n_trainï¼Œn_train)ï¼Œæ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„è®­ç»ƒè¾“å…¥
X_tile = x_train.repeat((n_train, 1))
# Y_tileçš„å½¢çŠ¶:(n_trainï¼Œn_train)ï¼Œæ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„è®­ç»ƒè¾“å‡º
Y_tile = y_train.repeat((n_train, 1))
# keysçš„å½¢çŠ¶:('n_train'ï¼Œ'n_train'-1)
keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))
# valuesçš„å½¢çŠ¶:('n_train'ï¼Œ'n_train'-1)
values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))

"""
    [è®­ç»ƒå¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èšæ¨¡å‹]æ—¶ï¼Œä½¿ç”¨å¹³æ–¹æŸå¤±å‡½æ•°å’Œéšæœºæ¢¯åº¦ä¸‹é™ã€‚
"""
net = NWKernelRegression()
loss = nn.MSELoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=0.5)
animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])

for epoch in range(5):
    trainer.zero_grad()
    l = loss(net(x_train, keys, values), y_train)
    l.sum().backward()
    trainer.step()
    print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')
    animator.add(epoch + 1, float(l.sum()))

"""
    å¦‚ä¸‹æ‰€ç¤ºï¼Œè®­ç»ƒå®Œå¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èšæ¨¡å‹åå¯ä»¥å‘ç°ï¼š 
    åœ¨å°è¯•æ‹Ÿåˆå¸¦å™ªå£°çš„è®­ç»ƒæ•°æ®æ—¶ï¼Œ [é¢„æµ‹ç»“æœç»˜åˆ¶]çš„çº¿ä¸å¦‚ä¹‹å‰éå‚æ•°æ¨¡å‹çš„å¹³æ»‘ã€‚
"""

# keysçš„å½¢çŠ¶:(n_testï¼Œn_train)ï¼Œæ¯ä¸€è¡ŒåŒ…å«ç€ç›¸åŒçš„è®­ç»ƒè¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œç›¸åŒçš„é”®ï¼‰
keys = x_train.repeat((n_test, 1))
# valueçš„å½¢çŠ¶:(n_testï¼Œn_train)
values = y_train.repeat((n_test, 1))
y_hat = net(x_test, keys, values).unsqueeze(1).detach()
plot_kernel_reg(y_hat)

"""
    ä¸ºä»€ä¹ˆæ–°çš„æ¨¡å‹æ›´ä¸å¹³æ»‘äº†å‘¢ï¼Ÿ ä¸‹é¢çœ‹ä¸€ä¸‹è¾“å‡ºç»“æœçš„ç»˜åˆ¶å›¾ï¼š 
    ä¸éå‚æ•°çš„æ³¨æ„åŠ›æ±‡èšæ¨¡å‹ç›¸æ¯”ï¼Œ å¸¦å‚æ•°çš„æ¨¡å‹åŠ å…¥å¯å­¦ä¹ çš„å‚æ•°åï¼Œ [æ›²çº¿åœ¨æ³¨æ„åŠ›æƒé‡è¾ƒå¤§çš„åŒºåŸŸå˜å¾—æ›´ä¸å¹³æ»‘]ã€‚
"""

d2l.show_heatmaps(net.attention_weights.unsqueeze(0).unsqueeze(0),
                  xlabel='Sorted training inputs',
                  ylabel='Sorted testing inputs')

"""
    å°ç»“
    Nadaraya-Watsonæ ¸å›å½’æ˜¯å…·æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„æœºå™¨å­¦ä¹ èŒƒä¾‹ã€‚
    Nadaraya-Watsonæ ¸å›å½’çš„æ³¨æ„åŠ›æ±‡èšæ˜¯å¯¹è®­ç»ƒæ•°æ®ä¸­è¾“å‡ºçš„åŠ æƒå¹³å‡ã€‚
    ä»æ³¨æ„åŠ›çš„è§’åº¦æ¥çœ‹ï¼Œåˆ†é…ç»™æ¯ä¸ªå€¼çš„æ³¨æ„åŠ›æƒé‡å–å†³äºå°†å€¼æ‰€å¯¹åº”çš„é”®å’ŒæŸ¥è¯¢ä½œä¸ºè¾“å…¥çš„å‡½æ•°ã€‚
    æ³¨æ„åŠ›æ±‡èšå¯ä»¥åˆ†ä¸ºéå‚æ•°å‹å’Œå¸¦å‚æ•°å‹ã€‚
"""
