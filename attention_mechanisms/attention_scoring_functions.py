"""
æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°

    ./nadaraya_waston.pyä½¿ç”¨äº†é«˜æ–¯æ ¸æ¥å¯¹æŸ¥è¯¢å’Œé”®ä¹‹é—´çš„å…³ç³»å»ºæ¨¡ã€‚ 
    ./nadaraya_waston.py å…¬å¼(3) ä¸­çš„ é«˜æ–¯æ ¸æŒ‡æ•°éƒ¨åˆ†å¯ä»¥è§†ä¸ºæ³¨æ„åŠ›è¯„åˆ†å‡½æ•°ï¼ˆattention scoring functionï¼‰ï¼Œ ç®€ç§°è¯„åˆ†å‡½æ•°ï¼ˆscoring functionï¼‰ï¼Œ 
        ç„¶åŽæŠŠè¿™ä¸ªå‡½æ•°çš„è¾“å‡ºç»“æžœè¾“å…¥åˆ°softmaxå‡½æ•°ä¸­è¿›è¡Œè¿ç®—ã€‚ 
        é€šè¿‡ä¸Šè¿°æ­¥éª¤ï¼Œå°†å¾—åˆ°ä¸Žé”®å¯¹åº”çš„å€¼çš„æ¦‚çŽ‡åˆ†å¸ƒï¼ˆå³æ³¨æ„åŠ›æƒé‡ï¼‰ã€‚ 
        æœ€åŽï¼Œæ³¨æ„åŠ›æ±‡èšçš„è¾“å‡ºå°±æ˜¯åŸºäºŽè¿™äº›æ³¨æ„åŠ›æƒé‡çš„å€¼çš„åŠ æƒå’Œã€‚

    ä»Žå®è§‚æ¥çœ‹ï¼Œä¸Šè¿°ç®—æ³•å¯ä»¥ç”¨æ¥å®žçŽ° ./attention_cues.py ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æ¡†æž¶ã€‚ 
    ä¸‹å›¾è¯´æ˜Žäº† å¦‚ä½•å°†æ³¨æ„åŠ›æ±‡èšçš„è¾“å‡ºè®¡ç®—æˆä¸ºå€¼çš„åŠ æƒå’Œï¼Œ å…¶ä¸­ ð‘Ž è¡¨ç¤ºæ³¨æ„åŠ›è¯„åˆ†å‡½æ•°ã€‚ 
    ç”±äºŽæ³¨æ„åŠ›æƒé‡æ˜¯æ¦‚çŽ‡åˆ†å¸ƒï¼Œ å› æ­¤åŠ æƒå’Œå…¶æœ¬è´¨ä¸Šæ˜¯åŠ æƒå¹³å‡å€¼ã€‚
                                                
                                                 -----        -----                                             
                                                 | + |  --->  |   | output                  
                                    softmax      -----        -----
                          æ³¨æ„åŠ›     -----         |                                                                                              
                          è¯„åˆ†å‡½æ•°   |   |         |        
        é”®(éžæ„å¿—çº¿ç´¢)               |   |         |                                                                                 
            -----         -----      |   |       --|--       -----                                                                                     
            |   |   --->  | a | ---> |   | --->  | x |  <--- |   |                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            |   |   --->  | a | ---> |   | --->  | x |  <--- |   |                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            |   |   --->  | a | ---> |   | --->  | x |  <--- |   |                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            |   |   --->  | a | ---> |   | --->  | x |  <--- |   |                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            |   |   --->  | a | ---> |   | --->  | x |  <--- |   |                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            -----    |    -----      |   |       --|--       -----                                                                                         
            |   |   --->  | a | ---> |   | --->  | x |  <--- |   |                                                                                         
            -----    |    -----      |   |       -----       -----                                                                                     
                     |               |   |                                                                                           
                     |               -----                                                                                    
                     |                                                                                                    
                     |                                                                                                    
            -----    |                                                                                                         
            |   |   --                                                                                                         
            -----                                                                                                                    
        æŸ¥è¯¢(æ„å¿—çº¿ç´¢)                                                                                                               

   ç”¨æ•°å­¦è¯­è¨€æè¿°ï¼Œå‡è®¾æœ‰ä¸€ä¸ªæŸ¥è¯¢ ðªâˆˆâ„^ð‘ž å’Œ ð‘š ä¸ªâ€œé”®ï¼å€¼â€å¯¹ (ð¤1,ð¯1),â€¦,(ð¤ð‘š,ð¯ð‘š)ï¼Œ å…¶ä¸­ ð¤ð‘–âˆˆâ„ð‘˜ï¼Œ ð¯ð‘–âˆˆâ„ð‘£ã€‚ 
   æ³¨æ„åŠ›æ±‡èšå‡½æ•° ð‘“ å°±è¢«è¡¨ç¤ºæˆå€¼çš„åŠ æƒå’Œï¼š
            
                               m
        ð‘“(ðª,(ð¤1,ð¯1),â€¦,(ð¤ð‘š,ð¯ð‘š))=âˆ‘ð›¼(ðª,ð¤ð‘–)ð¯ð‘– âˆˆâ„ð‘£, (1)
                              i=1

    å…¶ä¸­æŸ¥è¯¢ ðª å’Œé”® ð¤ð‘– çš„æ³¨æ„åŠ›æƒé‡ï¼ˆæ ‡é‡ï¼‰ æ˜¯é€šè¿‡æ³¨æ„åŠ›è¯„åˆ†å‡½æ•° ð‘Ž å°†ä¸¤ä¸ªå‘é‡æ˜ å°„æˆæ ‡é‡ï¼Œ å†ç»è¿‡softmaxè¿ç®—å¾—åˆ°çš„ï¼š

                                              m
        ð›¼(ðª,ð¤ð‘–)=softmax(ð‘Ž(ðª,ð¤ð‘–))=exp(ð‘Ž(ðª,ð¤ð‘–))/âˆ‘exp(ð‘Ž(ðª,ð¤ð‘—)) âˆˆâ„. (2)
                                             j=1

    æ­£å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œé€‰æ‹©ä¸åŒçš„æ³¨æ„åŠ›è¯„åˆ†å‡½æ•° ð‘Ž ä¼šå¯¼è‡´ä¸åŒçš„æ³¨æ„åŠ›æ±‡èšæ“ä½œã€‚ 
    æœ¬èŠ‚å°†ä»‹ç»ä¸¤ä¸ªæµè¡Œçš„è¯„åˆ†å‡½æ•°ï¼Œç¨åŽå°†ç”¨ä»–ä»¬æ¥å®žçŽ°æ›´å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚
"""
import math
import torch
from torch import nn
from d2l import torch as d2l

"""
    [æŽ©è”½softmaxæ“ä½œ]
    
    æ­£å¦‚ä¸Šé¢æåˆ°çš„ï¼Œsoftmaxæ“ä½œç”¨äºŽè¾“å‡ºä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒä½œä¸ºæ³¨æ„åŠ›æƒé‡ã€‚ 
    åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¹¶éžæ‰€æœ‰çš„å€¼éƒ½åº”è¯¥è¢«çº³å…¥åˆ°æ³¨æ„åŠ›æ±‡èšä¸­ã€‚ 
        ä¾‹å¦‚ï¼Œä¸ºäº†åœ¨ :numref:sec_machine_translation(æ„Ÿè§‰åƒæ˜¯æ–‡æœ¬å¤„ç†) ä¸­é«˜æ•ˆå¤„ç†å°æ‰¹é‡æ•°æ®é›†ï¼Œ æŸäº›æ–‡æœ¬åºåˆ—è¢«å¡«å……äº†æ²¡æœ‰æ„ä¹‰çš„ç‰¹æ®Šè¯å…ƒã€‚ 
        ä¸ºäº†ä»…å°†æœ‰æ„ä¹‰çš„è¯å…ƒä½œä¸ºå€¼æ¥èŽ·å–æ³¨æ„åŠ›æ±‡èšï¼Œ å¯ä»¥æŒ‡å®šä¸€ä¸ªæœ‰æ•ˆåºåˆ—é•¿åº¦ï¼ˆå³è¯å…ƒçš„ä¸ªæ•°ï¼‰ï¼Œ ä»¥ä¾¿åœ¨è®¡ç®—softmaxæ—¶è¿‡æ»¤æŽ‰è¶…å‡ºæŒ‡å®šèŒƒå›´çš„ä½ç½®ã€‚ 
        ä¸‹é¢çš„masked_softmaxå‡½æ•° å®žçŽ°äº†è¿™æ ·çš„æŽ©è”½softmaxæ“ä½œï¼ˆmasked softmax operationï¼‰ï¼Œ å…¶ä¸­ä»»ä½•è¶…å‡ºæœ‰æ•ˆé•¿åº¦çš„ä½ç½®éƒ½è¢«æŽ©è”½å¹¶ç½®ä¸º0ã€‚
"""
#@save
def masked_softmax(X, valid_lens):
    """é€šè¿‡åœ¨æœ€åŽä¸€ä¸ªè½´ä¸ŠæŽ©è”½å…ƒç´ æ¥æ‰§è¡Œsoftmaxæ“ä½œ"""
    # X:3Då¼ é‡ï¼Œvalid_lens:1Dæˆ–2Då¼ é‡
    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)
        # æœ€åŽä¸€è½´ä¸Šè¢«æŽ©è”½çš„å…ƒç´ ä½¿ç”¨ä¸€ä¸ªéžå¸¸å¤§çš„è´Ÿå€¼æ›¿æ¢ï¼Œä»Žè€Œå…¶softmaxè¾“å‡ºä¸º0
        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,
                              value=-1e6)
        return nn.functional.softmax(X.reshape(shape), dim=-1)

"""
    ä¸ºäº†[æ¼”ç¤ºæ­¤å‡½æ•°æ˜¯å¦‚ä½•å·¥ä½œ]çš„ï¼Œ è€ƒè™‘ç”±ä¸¤ä¸ª 2Ã—4 çŸ©é˜µè¡¨ç¤ºçš„æ ·æœ¬ï¼Œ è¿™ä¸¤ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦åˆ†åˆ«ä¸º 2 å’Œ 3ã€‚ 
    ç»è¿‡æŽ©è”½softmaxæ“ä½œï¼Œè¶…å‡ºæœ‰æ•ˆé•¿åº¦çš„å€¼éƒ½è¢«æŽ©è”½ä¸º0ã€‚
"""
masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))

"""
    åŒæ ·ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨äºŒç»´å¼ é‡ï¼Œä¸ºçŸ©é˜µæ ·æœ¬ä¸­çš„æ¯ä¸€è¡ŒæŒ‡å®šæœ‰æ•ˆé•¿åº¦ã€‚
"""
masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))

"""
[åŠ æ€§æ³¨æ„åŠ›]

    ä¸€èˆ¬æ¥è¯´ï¼Œå½“æŸ¥è¯¢å’Œé”®æ˜¯ä¸åŒé•¿åº¦çš„çŸ¢é‡æ—¶ï¼Œå¯ä»¥ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ›ä½œä¸ºè¯„åˆ†å‡½æ•°ã€‚ 
    ç»™å®šæŸ¥è¯¢ ðªâˆˆâ„^ð‘ž å’Œ é”® ð¤âˆˆâ„^ð‘˜ï¼Œ åŠ æ€§æ³¨æ„åŠ›ï¼ˆadditive attentionï¼‰çš„è¯„åˆ†å‡½æ•°ä¸º

        ð‘Ž(ðª,ð¤)=ð°_{ð‘£}^âŠ¤ tanh(ð–_{ð‘ž}ðª+ð–_{ð‘˜}ð¤) âˆˆ â„, (3)
 

    å…¶ä¸­å¯å­¦ä¹ çš„å‚æ•°æ˜¯ ð–_{ð‘ž} âˆˆ â„^(â„ŽÃ—ð‘ž)ã€ð–_{ð‘˜} âˆˆ â„^(â„ŽÃ—ð‘˜) å’Œ ð°_{ð‘£} âˆˆ â„^â„Žã€‚ 
    å¦‚ (3) æ‰€ç¤ºï¼Œ å°†æŸ¥è¯¢å’Œé”®è¿žç»“èµ·æ¥åŽè¾“å…¥åˆ°ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä¸­ï¼Œ æ„ŸçŸ¥æœºåŒ…å«ä¸€ä¸ªéšè—å±‚ï¼Œå…¶éšè—å•å…ƒæ•°æ˜¯ä¸€ä¸ªè¶…å‚æ•° â„Žã€‚ 
    é€šè¿‡ä½¿ç”¨ tanh ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œå¹¶ä¸”ç¦ç”¨åç½®é¡¹ã€‚

    ä¸‹é¢æ¥å®žçŽ°åŠ æ€§æ³¨æ„åŠ›ã€‚
"""
#@save
class AdditiveAttention(nn.Module):
    """åŠ æ€§æ³¨æ„åŠ›"""
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # åœ¨ç»´åº¦æ‰©å±•åŽï¼Œ
        # queriesçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œ1ï¼Œnum_hidden)
        # keyçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œ1ï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œnum_hiddens)
        # ä½¿ç”¨å¹¿æ’­æ–¹å¼è¿›è¡Œæ±‚å’Œ
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # self.w_vä»…æœ‰ä¸€ä¸ªè¾“å‡ºï¼Œå› æ­¤ä»Žå½¢çŠ¶ä¸­ç§»é™¤æœ€åŽé‚£ä¸ªç»´åº¦ã€‚
        # scoresçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œâ€œé”®-å€¼â€å¯¹çš„ä¸ªæ•°)
        scores = self.w_v(features).squeeze(-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # valuesçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œå€¼çš„ç»´åº¦)
        return torch.bmm(self.dropout(self.attention_weights), values)

"""
    ç”¨ä¸€ä¸ªå°ä¾‹å­æ¥[æ¼”ç¤ºä¸Šé¢çš„AdditiveAttentionç±»]ï¼Œ 
        å…¶ä¸­æŸ¥è¯¢ã€é”®å’Œå€¼çš„å½¢çŠ¶ä¸ºï¼ˆæ‰¹é‡å¤§å°ï¼Œæ­¥æ•°æˆ–è¯å…ƒåºåˆ—é•¿åº¦ï¼Œç‰¹å¾å¤§å°ï¼‰ï¼Œå®žé™…è¾“å‡ºä¸º (2,1,20) ã€ (2,10,2) å’Œ (2,10,4)ã€‚ 

        æ³¨æ„åŠ›æ±‡èšè¾“å‡ºçš„å½¢çŠ¶ä¸ºï¼ˆæ‰¹é‡å¤§å°ï¼ŒæŸ¥è¯¢çš„æ­¥æ•°ï¼Œå€¼çš„ç»´åº¦ï¼‰ã€‚
"""
queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))
# valuesçš„å°æ‰¹é‡ï¼Œä¸¤ä¸ªå€¼çŸ©é˜µæ˜¯ç›¸åŒçš„
values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)
valid_lens = torch.tensor([2, 6])

attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8, dropout=0.1)

attention.eval()
attention(queries, keys, values, valid_lens)

"""
    å°½ç®¡åŠ æ€§æ³¨æ„åŠ›åŒ…å«äº†å¯å­¦ä¹ çš„å‚æ•°ï¼Œä½†ç”±äºŽæœ¬ä¾‹å­ä¸­æ¯ä¸ªé”®éƒ½æ˜¯ç›¸åŒçš„ï¼Œ æ‰€ä»¥[æ³¨æ„åŠ›æƒé‡]æ˜¯å‡åŒ€çš„ï¼Œç”±æŒ‡å®šçš„æœ‰æ•ˆé•¿åº¦å†³å®šã€‚
"""
d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)), xlabel='Keys', ylabel='Queries')

"""
    [ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›]

    ä½¿ç”¨ç‚¹ç§¯å¯ä»¥å¾—åˆ°è®¡ç®—æ•ˆçŽ‡æ›´é«˜çš„è¯„åˆ†å‡½æ•°ï¼Œ ä½†æ˜¯ç‚¹ç§¯æ“ä½œè¦æ±‚æŸ¥è¯¢å’Œé”®å…·æœ‰ç›¸åŒçš„é•¿åº¦ ð‘‘ã€‚ 

    å‡è®¾æŸ¥è¯¢å’Œé”®çš„æ‰€æœ‰å…ƒç´ éƒ½æ˜¯ç‹¬ç«‹çš„éšæœºå˜é‡ï¼Œ å¹¶ä¸”éƒ½æ»¡è¶³é›¶å‡å€¼å’Œå•ä½æ–¹å·®ï¼Œ é‚£ä¹ˆä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯çš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º ð‘‘ã€‚ 
    ä¸ºç¡®ä¿æ— è®ºå‘é‡é•¿åº¦å¦‚ä½•ï¼Œ ç‚¹ç§¯çš„æ–¹å·®åœ¨ä¸è€ƒè™‘å‘é‡é•¿åº¦çš„æƒ…å†µä¸‹ä»ç„¶æ˜¯ 1 ï¼Œ æˆ‘ä»¬å†å°†ç‚¹ç§¯é™¤ä»¥ âˆšð‘‘ ï¼Œ åˆ™ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆscaled dot-product attentionï¼‰è¯„åˆ†å‡½æ•°ä¸ºï¼š

        ð‘Ž(ðª,ð¤)=ðªâŠ¤ð¤/âˆšð‘‘. (4)

    åœ¨å®žè·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä»Žå°æ‰¹é‡çš„è§’åº¦æ¥è€ƒè™‘æé«˜æ•ˆçŽ‡ï¼Œ ä¾‹å¦‚åŸºäºŽð‘›ä¸ªæŸ¥è¯¢å’Œð‘š ä¸ªé”®ï¼å€¼å¯¹è®¡ç®—æ³¨æ„åŠ›ï¼Œ å…¶ä¸­æŸ¥è¯¢å’Œé”®çš„é•¿åº¦ä¸ºð‘‘ï¼Œå€¼çš„é•¿åº¦ä¸ºð‘£ã€‚ 
    æŸ¥è¯¢ ð âˆˆ â„^(ð‘›Ã—ð‘‘) ã€ é”® ðŠ âˆˆ â„^(ð‘š Ã—ð‘‘) å’Œ å€¼ ð• âˆˆ â„^(ð‘š Ã— ð‘£) çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æ˜¯ï¼š

        softmax(ððŠâŠ¤/âˆšð‘‘)ð• âˆˆ â„^(ð‘›Ã—ð‘£). (5)

    ä¸‹é¢çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„å®žçŽ°ä½¿ç”¨äº†æš‚é€€æ³•è¿›è¡Œæ¨¡åž‹æ­£åˆ™åŒ–ã€‚
"""
#@save
class DotProductAttention(nn.Module):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # queriesçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œd)
    # keysçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œd)
    # valuesçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œå€¼çš„ç»´åº¦)
    # valid_lensçš„å½¢çŠ¶:(batch_sizeï¼Œ)æˆ–è€…(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # è®¾ç½®transpose_b=Trueä¸ºäº†äº¤æ¢keysçš„æœ€åŽä¸¤ä¸ªç»´åº¦
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)

"""
    ä¸ºäº†[æ¼”ç¤ºä¸Šè¿°çš„DotProductAttentionç±»]ï¼Œ æˆ‘ä»¬ä½¿ç”¨ä¸Žå…ˆå‰åŠ æ€§æ³¨æ„åŠ›ä¾‹å­ä¸­ç›¸åŒçš„é”®ã€å€¼å’Œæœ‰æ•ˆé•¿åº¦ã€‚ 
    å¯¹äºŽç‚¹ç§¯æ“ä½œï¼Œæˆ‘ä»¬ä»¤æŸ¥è¯¢çš„ç‰¹å¾ç»´åº¦ä¸Žé”®çš„ç‰¹å¾ç»´åº¦å¤§å°ç›¸åŒã€‚
"""
queries = torch.normal(0, 1, (2, 1, 2))
attention = DotProductAttention(dropout=0.5)
attention.eval()
attention(queries, keys, values, valid_lens)

"""
    ä¸ŽåŠ æ€§æ³¨æ„åŠ›æ¼”ç¤ºç›¸åŒï¼Œç”±äºŽé”®åŒ…å«çš„æ˜¯ç›¸åŒçš„å…ƒç´ ï¼Œ è€Œè¿™äº›å…ƒç´ æ— æ³•é€šè¿‡ä»»ä½•æŸ¥è¯¢è¿›è¡ŒåŒºåˆ†ï¼Œå› æ­¤èŽ·å¾—äº†[å‡åŒ€çš„æ³¨æ„åŠ›æƒé‡]ã€‚
"""
d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')

"""
å°ç»“
    å°†æ³¨æ„åŠ›æ±‡èšçš„è¾“å‡ºè®¡ç®—å¯ä»¥ä½œä¸ºå€¼çš„åŠ æƒå¹³å‡ï¼Œé€‰æ‹©ä¸åŒçš„æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°ä¼šå¸¦æ¥ä¸åŒçš„æ³¨æ„åŠ›æ±‡èšæ“ä½œã€‚
    å½“æŸ¥è¯¢å’Œé”®æ˜¯ä¸åŒé•¿åº¦çš„çŸ¢é‡æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å¯åŠ æ€§æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°ã€‚
    å½“å®ƒä»¬çš„é•¿åº¦ç›¸åŒæ—¶ï¼Œä½¿ç”¨ç¼©æ”¾çš„â€œç‚¹ï¼ç§¯â€æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°çš„è®¡ç®—æ•ˆçŽ‡æ›´é«˜ã€‚
"""
