PG
    1.NN as actor
    input: observation
    output: stochasticéšæœºçš„ probability of taking the action  ----|
            output layar : æœ‰å‡ ç§åŠ¨ä½œå°±æœ‰å‡ ä¸ªdimension             |
                                                                   |
    å¥½å¤„:                                                          |
        using network instead a lookup table
        generalize, ä¸¾ä¸€åä¸‰, æ²¡æœ‰è§è¿‡çš„ä¸œè¥¿ä¹Ÿèƒ½æœ‰è¾“å‡º             |
        ä¼ ç»Ÿæ–¹æ³•ï¼šQè¡¨ è¾“å…¥<->è¾“å‡º                                  |
                                                                   |
    2.actorçš„å¥½å:                                                 |
        è®©actorå»"ç©æ¸¸æˆ" -> total reward R = Î£ri                  |
        Even with the same actor, R is different each time         |
            Randomness in the actor and the game    ----------------
            actoré‡‡å–çš„åŠ¨ä½œ å’Œ æ¸¸æˆçš„æœ¬èº« æœ‰éšæœºæ€§
        æˆ‘ä»¬å¸Œæœ›maximize total reward R

        An episode is considered as a trajectory Ï„
            Ï„= {s1,a1,r1,s2,a2,r2,...,st,at,rt} ä¸€ä¸ªåºåˆ—
            R(Ï„) =  Î£ri
                    Ï„
            If you use an actor to play the game, each Ï„has a probability to be sampled
                The probability depends on actor parameter Î¸: P(Ï„|Î¸)   --- Î¸ å…¶å®å°±æ˜¯ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œç­–ç•¥ç»™å®šäº†å¯ä»¥è®¡ç®—æ¯ä¸€ä¸ªtrajectory Ï„ å‘ç”Ÿçš„å‡ ç‡

                E[R_{Î¸}] æ€»å¥–åŠ±çš„æœŸæœ› =  Î£ R(Ï„) P(Ï„|Î¸) 
                                         Ï„
                ç©·ä¸¾æ‰€æœ‰ Ï„ä¸ç°å®
                è®©actorå®ŒNåœº å¾—åˆ°{Ï„1, Ï„2, ..., Ï„N}
                             N
                è¿‘ä¼¼= 1/N *  Î£ R(Ï„i)
                            i=1

    3.Gradient Ascent   Gradient Ascentï¼ˆæ¢¯åº¦ä¸Šå‡ï¼‰æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºæœ€å¤§åŒ–ä¸€ä¸ªå‡½æ•°çš„å€¼ã€‚
        æ­¥éª¤ï¼š
            ä»åˆå§‹ç‚¹ ğ‘¥0 å¼€å§‹ã€‚
            è®¡ç®—å½“å‰ç‚¹ ğ‘¥ğ‘˜ å¤„çš„æ¢¯åº¦ âˆ‡ğ‘“(ğ‘¥ğ‘˜)ã€‚
            æ²¿ç€æ¢¯åº¦çš„æ–¹å‘ï¼ˆæ­£æ–¹å‘ï¼‰æ›´æ–°å‚æ•°ã€‚
            é‡å¤ç›´åˆ°æ”¶æ•›ï¼ˆå³ï¼Œæ¢¯åº¦å˜å¾—è¶³å¤Ÿå°æˆ–è€…è¾¾åˆ°é¢„è®¾çš„æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰ã€‚

        æœ€å¤§è¯ E[R] å…¶å®å°±æ˜¯è°ƒæ•´NNçš„Î¸

        éšæœºÎ¸ (åˆå§‹actor)
                                                                                                              N
        âˆ‡E[R_{Î¸}] = Î£ R(Ï„) âˆ‡P(Ï„|Î¸) = Î£ R(Ï„) P(Ï„|Î¸) âˆ‡P(Ï„|Î¸) / P(Ï„|Î¸) =  Î£ R(Ï„) P(Ï„|Î¸) âˆ‡logP(Ï„|Î¸)   è¿‘ä¼¼= 1/N * Î£ R(Ï„) P(Ï„|Î¸) âˆ‡logP(Ï„|Î¸)
                    Ï„                Ï„                                 Ï„                                     i=1

        P(Ï„|Î¸) = P(s1)P(a1|s1, Î¸)P(r1, s2|s1, a1)P(a2|s2, Î¸)P(a1|s1, Î¸)...
                       T
               = p(s1) Î  P(at|st, Î¸)P(rt, st|st, at)            å…¶ä¸­ åªæœ‰p(at|st, Î¸) è·Ÿactor(NN)æœ‰å…³ç³»
                      t=1

                               T
        logP(Ï„|Î¸) = logp(s1) + Î£ logP(at|st, Î¸) + logP(rt, st|st, at)
                              t=1

                     T                   
        âˆ‡logP(Ï„|Î¸) = Î£ logP(at|st, Î¸)
                    t=1                 



                              N                          N      T                         N  T                    
        âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£ R(Ï„) âˆ‡logP(Ï„|Î¸)  = 1/N * Î£ R(Ï„) Î£ âˆ‡logP(at|st, Î¸) = 1/N * Î£  Î£  R(Ï„) âˆ‡logP(at|st, Î¸)     æ³¨:æ˜¯ä¸æ˜¯å°‘äº†P(Ï„|Î¸)???
                             i=1                        i=1    t=1                       i=1t=1                   

        ç›´è§‰å°±æ˜¯: åœ¨Ï„i é‡‡å– at åœ¨st çŠ¶æ€ä¸‹ R(Ï„) æ˜¯æ­£çš„ï¼Œæ”¹å˜Î¸ å˜å¤§ P(at|st)
                                                  è´Ÿçš„ï¼Œæ”¹å˜Î¸ å˜å° P(at|st)

        æ³¨æ„è¿™é‡ŒR(Ï„) æ˜¯ä¸€ä¸ªepisodeçš„total reward

        åŠ logï¼Œå¾®åˆ†ä¹‹åé™¤æ‰ä¸€ä¸ªå‡ ç‡ åšä¸€ä¸ªæ­£åˆ™åŒ–

        add a baseline : å¦‚æœå…¨æ˜¯æ­£ï¼ŒæŸä¸ªactionæ²¡sampleåˆ° å‡ ç‡å°±ä¼šå‡å° æ­£åˆ™åŒ–

                                  N  T                    
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£  (R(Ï„) -b) âˆ‡logP(at|st, Î¸) 
                                 i=1t=1                   

        
        
        initialize (random Î¸) -> play episode (data collection) -> Gradient Ascent(Update Model)    å¾ªç¯
                                                                    
                                                                                                                       ^
                                                                   åœ¨classificationä¸­ æˆ‘ä»¬minimize corss enptory  : -Î£ yi logyi

                                                                                          ç­‰ä»·äº maximize logyi  å°±æ˜¯ ä¸Šé¢ logP(at|st, Î¸) åœ¨å½“å‰çŠ¶æ€é‡‡å–æŸåŠ¨ä½œçš„æ¦‚ç‡ å¯ä»¥çœ‹æˆåˆ†ç±»é—®é¢˜(é‡‡å–æŸä¸ªåŠ¨ä½œ)

                                                                                          ä½†æˆ‘ä»¬è¿˜ä¹˜ä¸Šäº† R(Ï„) å¯ä»¥çœ‹æˆ classification x å¯¹åº”çš„æ¬¡æ•° ï¼ˆ1 = 1æ¬¡ï¼‰ ä¹Ÿå¯ä»¥è½¬æ¢ä¸ºåˆ†ç±»é—®é¢˜
                                                                                                æ˜¯ä¸æ˜¯ä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯å¢åŠ å¥–åŠ±å¤§çš„çŠ¶æ€ä¸‹é‡‡å–çš„åŠ¨ä½œ

æ„Ÿæ‚Ÿï¼šNN(actor)çš„å‚æ•°ä½œä¸ºç­–ç•¥ï¼Œè¾“å…¥æ˜¯çŠ¶æ€ï¼Œè¾“å‡ºçš„åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
      çŸ¥é“æ¢¯åº¦æ›´æ–°çš„å…¬å¼äº†
                                  N  T                    
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£  R(Ï„)âˆ‡logP(at|st, Î¸) 
                                 i=1t=1                   
      å…ˆç”¨actoræ”¶é›†ä¸€å¤§å †(s, a), å’Œæ¯ä¸ªtrajectory çš„ total Reward
      ä»£å…¥æ¢¯åº¦æ›´æ–°å…¬å¼ æŠŠgradient ç®—å‡ºæ¥
        å¢åŠ å¥–åŠ±å¤§çš„çŠ¶æ€ä¸‹çš„é‡‡å–çš„åŠ¨ä½œçš„æ¦‚ç‡
      æ›´æ–°Î¸

      å°±æ˜¯æ”¶é›†æ•°æ® -> æ›´æ–°æ¨¡å‹ å¾ªç¯
      
      å¯ä»¥æ˜¯classification ä½†lossfunctionè¦ä¹˜è´Ÿå·å’Œtotal reward

å¦‚æœç™¼ç¾ä¸èƒ½å¾®åˆ†å°±ç”¨ policy gradient ç¡¬ train ä¸€å‘  -- ç¯å¢ƒå’Œrewardæ˜¯é»‘ç›’å­

tips:
    1.add baseline      å¦‚æœå…¨æ˜¯æ­£ï¼ŒæŸä¸ªactionæ²¡sampleåˆ° å‡ ç‡å°±ä¼šå‡å° æ­£åˆ™åŒ–

                                  N  T                    
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£  (R(Ï„) -b) âˆ‡logP(at|st, Î¸)           b å¯ä»¥å– E[R]
                                 i=1t=1                   

    2.Assign Suitable Credit
        åº”è¯¥ç»™æ¯ä¸€ä¸ªactionåˆé€‚çš„credit
        åœ¨ä¸€å¥æ¸¸æˆä¸­å¯èƒ½æœ‰çš„actionæ˜¯å¥½çš„ æœ‰çš„æ˜¯ä¸å¥½çš„
        æŠŠ(R(Ï„) -b)å…¨å±€çš„rewardæ¢æˆä»è¿™ä¸ªactionæ‰§è¡Œä¹‹åçš„reward

                                  N  T   T                
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£ ( Î£r -b) âˆ‡logP(at|st, Î¸)           b å¯ä»¥å– E[R]
                                 i=1t=1 t'=t              

        æ›´è¿‘ä¸€æ­¥
            æœªæ¥çš„rewardåšä¸€ä¸ªdiscount  æ—¶é—´æ‹–å¾—è¶Šé•¿ å½±å“åŠ›å°±è¶Šå°

                                  N  T   T                  
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£ ( Î£ Î³^(t' - t)r -b) âˆ‡logP(at|st, Î¸)           b å¯ä»¥å– E[R] ä¹Ÿå¯ä»¥æ˜¯state-dependent
                                 i=1t=1 t'=t              
                                        ------------------
                                        Advantage Function AÎ¸(st, at)
                                        How good it is if we take afother than other actions at st.
                                        Estimated by "critic" (later)

                                  N  T                      
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£ AÎ¸(st, at) âˆ‡logP(at|st, Î¸)           
                                 i=1t=1                   
                                                          
        
critic
    represented by a neural network (NN)
    input:
        state
    output:
        VÏ€(s) : When using actor Ï€, the cumulated reward expects to be obtained after seeing observation (state) s.  æ³¨æ„æ˜¯ä¼°è®¡ç›´åˆ°ç»“æŸä¼šè·å¾—çš„reward
        ç»™ä»–ä¸åŒçš„actorï¼ŒåŒä¸€ä¸ªstateï¼Œè¾“å‡ºä¹Ÿæ˜¯ä¸ä¸€æ ·çš„

    A critic does not determine the action.

    Given an actor Ï€, it evaluates the how good the actor is.

    å¦‚ä½•è®¡ç®—VÏ€(s):
        1.è’™ç‰¹å¡æ´›:é‡‡æ ·ï¼Œä¼°è®¡(é€¼è¿‘å®é™…æ•°å€¼) regressioné—®é¢˜ éœ€è¦å®Œæ•´å›åˆ
        2.Temporal-Difference (TD) : ä¸éœ€è¦ç­‰å¾…ä¸€ä¸ªå®Œæ•´çš„å›åˆï¼ˆepisodeï¼‰ç»“æŸæ‰èƒ½æ›´æ–°ä»·å€¼ä¼°è®¡ï¼Œè€Œæ˜¯é€šè¿‡é€æ­¥æ›´æ–°æ¥è¿›è¡Œå­¦ä¹ ã€‚
            TDæ–¹æ³•çš„æ›´æ–°ä¾æ®æ˜¯ å½“å‰æ—¶åˆ»çš„ä¼°è®¡ä¸ä¸‹ä¸€ä¸ªæ—¶åˆ»çš„ä¼°è®¡ä¹‹é—´çš„å·®å¼‚ï¼Œè¿™å°±æ˜¯â€œæ—¶é—´å·®å¼‚â€ã€‚
            ...st, at, rt, st+1....
            VÏ€(st) = VÏ€(st+1) + rt
            è®­ç»ƒNN è®©VÏ€(st) - VÏ€(st+1) é€¼è¿‘ rt


Another Critic
    State-action value function QÏ€(s, a)

    When using actor Ï€, the cumulated reward expects to be obtained after seeing observation s and taking a

    input:
        (s, a) or s (actionç©ºé—´å¾ˆå°çš„æƒ…å†µ)

    output:
        the cumulated reward expects to be obtained after seeing observation (state) s and taking action a 
        QÏ€(s, a)
        or
        the cumulated reward expects to be obtained after seeing observation (state) s and taking action a(actionç©ºé—´å¾ˆå°çš„æƒ…å†µä¸‹ï¼Œå±•å¼€æ‰€æœ‰actionï¼Œè¿™æ ·è¾“å…¥çš„æ—¶å€™å°±ä¸éœ€è¦actionäº†)
        QÏ€(s, a = left) QÏ€(s, a = right) QÏ€(s, a = fire)


Another Way to use Critic: Q-Learning

loop:
    Ï€ interacts with the environment

            |               1.è’™ç‰¹å¡æ´›                    
            v               2.Temporal-Difference (TD) 

    Learning QÏ€(s,a)

            |               "Better":VÏ€'(st) >= VÏ€(st) for all state s
            v               Ï€'(s) = arg maxQÏ€(s, a)
                                         a
                            problem:Not suitable for continuous action a (solve it later) å› ä¸ºè¦ç©·ä¸¾æ‰€æœ‰çš„action
                            A solution : Pathwise Derivative Policy Gradient å¢åŠ ä¸€ä¸ªç½‘ç»œ(actor)äº§ç”Ÿè¿ç»­çš„åŠ¨ä½œ (DDPG)

                                               ----
                                          s -> |  |
                                |-----|        |QÏ€| -> QÏ€(s, a)
                            s ->|Actor|-> a -> |  |
                                | Ï€   |        ----
                                -------

    Find a new actor Ï€' "better" than Ï€    Ï€'=Ï€

trick:  easy-rlé‡Œé¢
    Double DQN
    Dueling DQN


Actor + Critic:
    Actor ä¸çœ‹ç¯å¢ƒçš„reward å» Gradient Ascent ç¯å¢ƒrewardçš„éšæœºæ€§å¤ªå¤§
          è·ŸCriticå­¦

    A2C Advantage Actor-Critic
    A3C Asynchronous Advantage Actor-Critic ï¼ˆå¼€åˆ†èº«åŠ é€Ÿå­¦ä¹ )


Inverse Reinforcement Learning
    env actor no reward function
    We have demonstration of the expert.
        Each Ï„ is a trajectory of the export.
        ä¸“å®¶ç©çš„å›åˆ

    => æ¨å¯¼å‡ºReward Function => å†ç”¨Reinforcement Learning çš„æ–¹æ³•

    Principle: The teacher is always the best.
    Basic idea:
        â€¢ Initialize an actor

        â€¢ In each iteration
            â€¢ The actor interacts with the environments to obtain some trajectories
            â€¢ Define a reward function, which makes the trajectories of the teacher better than the actor 
            â€¢ The actor learns to maximize the reward based on the new reward function.

        â€¢ Output the reward function and the actor learned from the reward function


On-policy v.s. Off-policy

â€¢ On-policy: The agent learned and the agent interacting with the environment is the same.

                              N                          N      T                         N  T                    
        âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£ R(Ï„) âˆ‡logP(Ï„|Î¸)  = 1/N * Î£ R(Ï„) Î£ âˆ‡logP(at|st, Î¸) = 1/N * Î£  Î£  R(Ï„) âˆ‡logP(at|st, Î¸)     æ³¨:æ˜¯ä¸æ˜¯å°‘äº†P(Ï„|Î¸)???
                             i=1                        i=1    t=1                       i=1t=1                   

        ç”¨actorä¸ç¯å¢ƒäº¤äº’é‡‡é›†æ•°æ®ï¼Œæ¨¡å‹å‚æ•°å˜äº†æ„å‘³ç€åœ¨æŸä¸ªçŠ¶æ€é‡‡å–åŠ¨ä½œçš„æ¦‚ç‡å˜äº†ï¼Œç­–ç•¥å˜äº† ä¹‹å‰é‡‡é›†çš„æ•°æ®å°±ä¸èƒ½ç”¨æ¥è®­ç»ƒæ¨¡å‹äº†  éå¸¸èŠ±æ—¶é—´ é‡‡é›†ä¸€æ¬¡æ•°æ® Gradient Ascent ä¸€æ¬¡
            Use Ï€Î¸ to collect data. When 0 is updated, we have to sample training data again.                  |
                                                                                                               |
â€¢ Off-policy: The agent learned and the agent interacting with the environment is different.                   |
                                                                                                               |
        ä»On-policy åˆ° Off-policy çš„å¥½å¤„å°±æ˜¯:                                                                  |
            Goal: Using the sample from Ï€Î¸', to train 0. Î¸' is fixed, so we can re-use the samiple data.    æ¯”è¾ƒæœ‰æ•ˆç‡

            Importance Sampling 
                                  N
            E_{x~p}[f(x)] çº¦= 1/N Î£  f(xi)                  xi is sample from p(x) but we only have xi sampled from q(x)
                                 i=1

                            = âˆ« f(x)p(x) dx = âˆ« f(x)p(x)q(x)/q(x) dx = E_{x~q}[f(x)p(x)/q(x)]

            Issue of Importance Sampling:
                æ–¹å·®ä¸ä¸€æ ·  æ‰€ä»¥å¦‚æœsampleæ¬¡æ•°ä¸å¤Ÿå¤šï¼Œå¯èƒ½æœŸæœ›å·®è·å¤§

            ç”¨äºä»On-policy åˆ° Off-policy å˜æ¢ ä¸æ‹¿actorä¸ç¯å¢ƒåšäº’åŠ¨


        âˆ‡E[R_{Î¸}] = E_{Ï„~pÎ¸(Ï„)}[R(Ï„) âˆ‡logP(Ï„|Î¸)]

                  = E_{Ï„~pÎ¸'(Ï„)}[R(Ï„) âˆ‡logP(Ï„|Î¸) pÎ¸(Ï„) / pÎ¸'(Ï„)]

            Sample the data from 0'.

            Use the data to train 0 many times.

                                         
        âˆ‡E[R_{Î¸}] = E_{(st, at) ~ Ï€Î¸}[AÎ¸(st, at) âˆ‡logP(at|st, Î¸)]

                  = E_{(st, at) ~ Ï€Î¸'}[AÎ¸'(st, at) âˆ‡logP(at|st, Î¸) pÎ¸(Ï„) / pÎ¸'(Ï„)]
                                       
                  = E_{(st, at) ~ Ï€Î¸'}[AÎ¸'(st, at) âˆ‡logP(at|st, Î¸) pÎ¸(at|st)pÎ¸(st) / pÎ¸'(at|st)pÎ¸'(st)  ]   æ³¨æ„ï¼špÎ¸(st) / pÎ¸'(st) è¿™ä¸€é¡¹ä¸å¥½ç®— æå®æ¯…è¯´è¯´æœè‡ªå·±è®¤ä¸ºæ²¡å½±å“

        âˆ‡f(x)=f(x)âˆ‡logf(x)

        ==> JÎ¸'(Î¸) = E_{(st, at) ~ Ï€Î¸'}[AÎ¸'(st, at) pÎ¸(at|st) / pÎ¸'(at|st)]   åŸæ–¹ç¨‹ Optimizationç›®æ ‡å‡½æ•° æ±‚å¯¼åç­‰äºä¸Šé¢ âˆ‡E[R_{Î¸}] 
                                                                              è¿™ä¸€é¡¹æ˜¯å¯ä»¥ç®—çš„



        Proximal Policy Optimization (PPO)/TRPO

            JÎ¸'_PPO(Î¸) = JÎ¸'(Î¸) - Î²KL(Î¸, Î¸')    å› ä¸ºIssue of Importance Sampling: æ–¹å·®ä¸ä¸€æ ·  æ‰€ä»¥å¦‚æœsampleæ¬¡æ•°ä¸å¤Ÿå¤šï¼Œå¯èƒ½æœŸæœ›å·®è·å¤§ï¼Œè¿™é‡Œæœ‰ç‚¹æ­£åˆ™åŒ–çš„æ„Ÿè§‰

        PPO algorithm

            â€¢ Initial policy parameters 0Â°

            â€¢ In each iteration

                â€¢ Using Î¸^k to interact with the environment to collect {st, at} and compute advantage AÎ¸^k (st, at) Î¸^kå‰ä¸€ä¸ªtraining iterationå¾—åˆ°çš„æ¨¡å‹å‚æ•°
                â€¢ Find Î¸ optimizing  JÎ¸_PPO(Î¸)

                    JÎ¸^k_PPO(Î¸) = JÎ¸^k(Î¸) - Î²KL(Î¸, Î¸^k)    Update parameters several times

                    if KL(Î¸, Î¸^k) > KLmax, increase Î²   Adaptive KL Penalty
                    if KL(Î¸, Î¸^k) < KLmin, decrease Î² 

        PPO2

Q-Learning
    value-based
    learn Critic

    represented by a neural network (NN)
    input:
        state

    A critic does not determine the action.

    Given an actor Ï€, it evaluates the how good the actor is.

    output:
        VÏ€(s) : When using actor Ï€, the cumulated reward expects to be obtained after seeing observation (state) s.  æ³¨æ„æ˜¯ä¼°è®¡ç›´åˆ°ç»“æŸä¼šè·å¾—çš„reward

    å¼ºè°ƒä¸€ç‚¹:
        ä¸€ä¸ª Critic éƒ½æ˜¯ç»‘å®šä¸€ä¸ª Actor
        æ²¡æœ‰åŠæ³•å‡­ç©ºestimateä¸€ä¸ªV éƒ½æ˜¯ç»™å®šä¸€ä¸ªstateå‡è®¾æ¥ä¸‹æ¥äº’åŠ¨actoræ˜¯Ï€ï¼Œä¼šè·å¾—å¤šå°‘reward
        The output values of a critic depend gn the actor evaluated.
        ç»™ä»–ä¸åŒçš„actorï¼ŒåŒä¸€ä¸ªstateï¼Œè¾“å‡ºä¹Ÿæ˜¯ä¸ä¸€æ ·çš„
            vä»¥å‰çš„é˜¿å…‰ï¼ˆå¤§é¦¬æ­¥é£›ï¼‰=bad 
            vğ¤“–å¼·çš„é˜¿å…‰ï¼ˆå¤§é¦¬æ­¥é£›ï¼‰=good

    How to estimate VÏ€(s)
        â€¢ Monte-Carlo (MC) based approach
            The critic watches Ï€ playing the game

                  NN            
                 |--|           
            s -> |VÏ€| -> VÏ€(s) <-> G(accumulate reward éœ€è¦ä¸€æ•´ä¸ªå›åˆ)    (regression problem)
                 |--|           

            After seeing Sa, Until the end of the episode, the cumulated reward is Ga
            After seeing Sb, Until the end of the episode, the cumulated reward is Gb

        â€¢ Temporal-difference (TD) approach
                    ...st, at, rt, st+1,...
                    VÏ€(st) = VÏ€(st+1) + rt

                           NN          
                          |--|         
                    st -> |VÏ€| -> VÏ€(st)
                          |--|         

                                                VÏ€(st) - VÏ€(st+1) <-> rt   (regression problem) 

                           NN          
                          |--|         
                    st -> |VÏ€| -> VÏ€(st+1)
                          |--|         
        MC v.s. TD

            MC
                Larger variance     Var[kX] = k^2Var[X]
                Ga is the summation of many steps

            TD
                r smaller variance
                VÏ€(s) may be inaccurate

    Another Critic
        State-action value function QÏ€(s, a)

        When using actor Ï€, the cumulated reward expects to be obtained after seeing observation s and taking a
        QÏ€(s, a) = acotråœ¨å½“å‰sé‡‡å–action a åé¢å°±ç”±actorè‡ªå·±å»ç© å°†ä¼šè·å¾—çš„æ€»å¥–åŠ±

        input:
            (s, a) or s (actionç©ºé—´å¾ˆå°çš„æƒ…å†µ)

        output:
            the cumulated reward expects to be obtained after seeing observation (state) s and taking action a 
            QÏ€(s, a)
            or
            the cumulated reward expects to be obtained after seeing observation (state) s and taking action a(actionç©ºé—´å¾ˆå°çš„æƒ…å†µä¸‹ï¼Œå±•å¼€æ‰€æœ‰actionï¼Œè¿™æ ·è¾“å…¥çš„æ—¶å€™å°±ä¸éœ€è¦actionäº†)
            QÏ€(s, a = left) QÏ€(s, a = right) QÏ€(s, a = fire)

                       ----
                  s -> |  |
                       |QÏ€| -> QÏ€(s, a)
                  a -> |  |
                       ----
            
                       ----
                       |  | -> QÏ€(s, a=left)
                  s -> |QÏ€| -> QÏ€(s, a=right)       for discrete action only
                       |  | -> QÏ€(s, a=fire)
                       ----

    loop:
        Ï€ interacts with the environment

                |               1.è’™ç‰¹å¡æ´›                    
                v               2.Temporal-Difference (TD) 

        Learning QÏ€(s,a)        QÏ€(s, a) = acotråœ¨å½“å‰sé‡‡å–action a åé¢å°±ç”±actorè‡ªå·±å»ç© å°†ä¼šè·å¾—çš„æ€»å¥–åŠ±

                |               "Better":VÏ€'(st) >= VÏ€(st) for all state s
                v               Ï€'(s) = arg maxQÏ€(s, a)
                                             a
                                Ï€' does not have extra parameters. It depends on Q

                                problem:Not suitable for continuous action a (solve it later) å› ä¸ºè¦ç©·ä¸¾æ‰€æœ‰çš„action
                                A solution : Pathwise Derivative Policy Gradient å¢åŠ ä¸€ä¸ªç½‘ç»œ(actor)äº§ç”Ÿè¿ç»­çš„åŠ¨ä½œ (DDPG)

                                                   ----
                                              s -> |  |
                                    |-----|        |QÏ€| -> QÏ€(s, a)
                                s ->|Actor|-> a -> |  |
                                    | Ï€   |        ----
                                    -------

        Find a new actor Ï€' "better" than Ï€    Ï€'=Ï€     ä¸€ç›´å¾ªç¯ä¸‹å»policyå°±ä¼šæ›´å¥½
        
    tips:

        Target Network
            ...st, at, rt, st+1,...
            QÏ€(st, at) = QÏ€(st+1, at+1) + rt

                   NN          
            st -> |--|         
                  |QÏ€| -> QÏ€(st, at)
            at -> |--|         

                                        QÏ€(st+1, Ï€(st+1)) - QÏ€(st, at) <-> rt   (regression problem) 
                                                                      å»é€¼è¿‘
                      NN          
            st+1  -> |--|         
                     |QÏ€| -> QÏ€(st+1, Ï€(st+1))
          Ï€(st+1) -> |--|         

            ä¸å¥½trainingï¼Œå› ä¸ºæ›´æ–°ç½‘ç»œå‚æ•°ï¼Œtargetä¹Ÿä¸€ç›´åœ¨å˜

                å›ºå®šç›®æ ‡ç½‘ç»œ, After updating N times QÏ€(Ï€è¡¨ç¤ºç¥ç»ç½‘ç»œçš„æ€§èƒ½å‚æ•°), Target Network = QÏ€

                          NN    Target Network         
                st+1  -> |--|         
                         |QÏ€| -> QÏ€(st+1, Ï€(st+1))  fixed value
              Ï€(st+1) -> |--|         

                         fixed

        Exploration
            The policy is based on Q-function

                a = arg maxQÏ€(s, a)
                         a
            
            ä¸PGçš„åŒºåˆ«æ˜¯PGè¾“å‡ºåŠ¨ä½œçš„probability distribution
            QÏ€è¾“å‡ºQ(s) or (Q(s), a)
            Q-Learning çš„policyæ˜¯é‡‡å–Qå€¼æœ€å¤§çš„åŠ¨ä½œ
                This is not a good way for data collection.

              -> a1     Q(s, a) = 0 Never explore
            s -> a2     Q(s, a) = 1 Always sampled
              -> a3     Q(s, a) = 0 Never explore


            Epsilon Greedy  Îµ would decay during learning
            
                        arg maxQ(s, a),         with probability 1 - Îµ
                a   =        a
                        random,                 otherwise

            Boltzmann Exploration
                    
                P(a|s) = exp(Q(s, a)) / âˆ‘ exp(Q(s, a))

        Replay Buffer
            
            Ï€ interacts with the environment    Put the experience into buffer.((st, at, rt, st+1),...)
                                                The experience in the buffer comes from different policies. å˜æˆäº†Off-policy
                                                Drop the old experience if the buffer is full.

                    |               In each iteration:
                    |                   1. Sample a batch
                    |                   2. Update Q- function
                    |               1.è’™ç‰¹å¡æ´›                    
                    v               2.Temporal-Difference (TD) 

            Learning QÏ€(s,a)        QÏ€(s, a) = acotråœ¨å½“å‰sé‡‡å–action a åé¢å°±ç”±actorè‡ªå·±å»ç© å°†ä¼šè·å¾—çš„æ€»å¥–åŠ±

                    |               "Better":VÏ€'(st) >= VÏ€(st) for all state s
                    v               Ï€'(s) = arg maxQÏ€(s, a)
                                                 a
                                    Ï€' does not have extra parameters. It depends on Q

                                    problem:Not suitable for continuous action a (solve it later) å› ä¸ºè¦ç©·ä¸¾æ‰€æœ‰çš„action
                                    A solution : Pathwise Derivative Policy Gradient å¢åŠ ä¸€ä¸ªç½‘ç»œ(actor)äº§ç”Ÿè¿ç»­çš„åŠ¨ä½œ (DDPG)

                                                       ----
                                                  s -> |  |
                                        |-----|        |QÏ€| -> QÏ€(s, a)
                                    s ->|Actor|-> a -> |  |
                                        | Ï€   |        ----
                                        -------

            Find a new actor Ï€' "better" than Ï€    Ï€'=Ï€     ä¸€ç›´å¾ªç¯ä¸‹å»policyå°±ä¼šæ›´å¥½

            Advantage:
                1.èŠ‚çº¦æ—¶é—´ å‡å°‘ä¸ç¯å¢ƒäº¤äº’çš„æ¬¡æ•°
                2.batch data diverse
                
                æˆ‘ä»¬æ˜¯ä¼°è®¡QÏ€(s, a), buffer ä¸­æ··æ‚äº†ä¸æ˜¯Ï€çš„experience æœ‰æ²¡æœ‰å…³ç³»?æå®æ¯…è¯´æ²¡æœ‰


        Typical Q-Learning Algorithm

                                                         ^
            â€¢ Initialize Q-function Q, target Q-function Q = Q
            â€¢ In each episode
                â€¢ For each time step t
                â€¢ Given state st, take action a, based on Q (epsilon greedy)
                â€¢ Obtain reward rt, and reach new state st+1 
                â€¢ Store (st, at, rt, st+1) into butter
                â€¢ Sample (si, ai, ri, si+1) from buffer (usually a batch) 
                                      ^
                â€¢ Target y = ri + max Q(si+1, a)
                â€¢ Update the parameters of Q to make Q(si, ai) close to y (regression)
                                      ^
                â€¢ Every C steps reset Q = Q Created 

    tips:
        Double DQN
            Q value is usually over-estimated
            
                Q(st, at) <-> rt + max Q(st+1, a)
                                    a
                                    Tend to select the action that is over-estimated

            Double DQN: two functions Q and Q'  
                Target Network
                é€‰actionçš„Q functionå’Œç®—valueçš„Q functionä¸æ˜¯åŒä¸€ä¸ª
                è¡Œæ”¿å’Œç«‹æ³•åˆ†å¼€

                Q(st, at) <-> rt + max Q'(st+1, arg maxQ(st+1, a))
                                                     a
                                    If Q over-estimate a, so it is selected. Q' would give it proper value.
                                    How about Q' overestimate? The action will not be selected by Q.

        Dueling DQN
            æ”¹äº†networkçš„æ¶æ„
            old output: Q(s, a)
            new output: Q(s, a) = A(s, a)                            + V(s)
                                  Vector(æ¯ä¸€ä¸ªactionéƒ½æœ‰ä¸€ä¸ªvalue)    Sclar
            
                                        
                                            state
                                          3  3  3  1
                Q(s, a)          action   1 -1  6  1
                                          2 -2  3  1

                   ||                         ||


                  V(s)                    2  0  4  1


                    +                          +

                                          1  3 -1  0
                 A(s, a)                 -1 -1  2  0
                                          0 -2 -1  0
                
                 æ”¹å˜V(s)ä¼šæ”¹å˜æ”¹åˆ—çš„æ‰€æœ‰Q(s, a)
                 è¿™æ ·å°±ç®—æ²¡æœ‰sampleåˆ°çš„(s, a), ä¹Ÿå¯ä»¥update estimate
                 ä¼šæœ‰ä¸€äº›constrainï¼šè®©æ¨¡å‹æ›´å€¾å‘äºæ”¹å˜V(s), è€Œä¸æ˜¯A(s, a)
                                    V(s) Average of column
                                    A(s,a) sum of column = 0
                 å®åš:
                    Normalize A(s,a) before adding with V(s)!!!

        Prioritized Reply

                Experience Buffer -> (st, at, rt, st+1) batch -> estimate Q(s, a)(TD/MC) -> Ï€'=Ï€ 

                ```
                    â€¢ Temporal-difference (TD) approach
                                    ...st, at, rt, st+1,...
                                    VÏ€(st) = VÏ€(st+1) + rt

                                           NN          
                                    st -> |--|         
                                          |QÏ€| -> Q(st, at)
                                    at -> |--|         
                                                                          ^                                                               
                                                      QÏ€(st, at) <-> rt + Q(st+1, at+1)   (regression problem) 
                                                                     ^
                                                      at+1 = arg max Q(st+1, a)
                                                                  a

                                           NN          
                                  st+1 -> |--|              
                                          |^ |    ^         
                                          |QÏ€| -> Q(st+1, at+1)
                                  at+1 -> |--|         

                ```

                The data with larger TD error in previous training has higher probability to be sampled.
                Parameter update procedure is also modified.


        Multi-step          Balance between MC and TD
            
                (st, at, rt, ..., st+N, at+N, rt+N, st+N+1) in Experience buffer
                ```
                    â€¢ Temporal-difference (TD) approach
                                    ...st, at, rt, st+1,...
                                    VÏ€(st) = VÏ€(st+1) + rt

                                           NN          
                                    st -> |--|         
                                          |QÏ€| -> Q(st, at)
                                    at -> |--|         
                                                                          t+N ^                                                               
                                                      QÏ€(st, at) <-> rt' + Î£  Q(st+N+1, at+N+1)   (regression problem) 
                                                                         t'=t
                                                                       ^
                                                      at+N+1 = arg max Q(st+N+1, a)
                                                                    a

                                             NN          
                                  st+N+1   -> |--|              
                                              |^ |    ^         
                                              |QÏ€| -> Q(st+1, at+1)
                                  at+N+1   -> |--|         

                ```

        Noisy Net

            â€¢ Noise on Action (Epsilon Greedy)

                Epsilon Greedy  Îµ would decay during learning
                
                            arg maxQ(s, a),         with probability 1 - Îµ
                    a   =        a
                            random,                 otherwise

                â€¢ Given the same state, the agent may takes different actions.
                â€¢ No real policy works in this way
                éš¨æ©Ÿäº‚è©¦

            â€¢ Noise on Parameters

                Inject noise into the parameters of Q-function at the beginning of each episode
                æ¯ä¸€å±€å¼€å§‹çš„æ—¶å€™åŠ noise, å¼€å§‹ä¹‹åä¸åŠ 
                â€¢ Given the same (similar) state, the agent takes the same action.
                    â€¢ â†’ State-dependent Exploration 
                â€¢ Explore in a consistent way
                æœ‰ç³»çµ±åœ°è©¦

                                        ~
                Q(s, a) -> add noise -> Q(s, a)

                            ~
                a = arg max Q(s,a )
                         a


        Distributional Q-function
            State-action value function QÏ€(s, a)
                When using actor Ï€, the cumulated reward expects to be obtained after seeing observation s and taking a
                QÏ€(s, a)æ˜¯ä¸€ä¸ªæœŸæœ›å€¼
                çœŸå®æ˜¯ä¸€ä¸ªdistribution
                Different distributions can have the same values.
                output æœŸæœ›å€¼expects -> output distributions

                
                           NN          
                          |--| -> QÏ€(s, a1)       
                    s ->  |QÏ€| -> QÏ€(s, a2)     A network with 3 outputs
                          |--| -> QÏ€(s, a3)        

                           NN          
                          |--| ->  distribution               
                    s ->  |QÏ€| ->  distribution A network with 15 outputs (each action has 5 bins, bins sum = 1)
                          |--| ->  distribution                               

        Rainbow
            æŠŠæ‰€æœ‰æ–¹æ³•åˆå¹¶èµ·æ¥

Q-Learning for Continuous Actions
        æ²¡æœ‰ppoä¹‹å‰ policy gradientæ¯”è¾ƒä¸ç¨³
        Q-Learningæ¯”è¾ƒå®¹æ˜“train åªè¦estimate ä¸€ä¸ªQ function å°±ä¸€å®šèƒ½æ‰¾åˆ°ä¸€ä¸ªæ›´å¥½çš„policy
                                    -----------------------
                                      regression problem æ¯”è¾ƒå®¹æ˜“
        ä½†Q-Learningä¸å¤ªå¥½å¤„ç†continuous problem, Action a is a continuous vector
            
            a = arg maxQ(s, a)
                     a

        Q-Learning estimate Q function ä¹‹å ä¸€å®šè¦è§£å‡ºæœ€å¤§çš„a è®©Qæœ€å¤§ å¦‚æœæ˜¯aæ˜¯continuousçš„ï¼Œä¸å¥½è§£ å¦‚æœæ˜¯ç¦»æ•£çš„ç›´æ¥å¸¦è¿›å»æ‰¾æœ€å¤§å€¼å°±è¡Œ

        Solution 1

            Sample a set of actions: {a1, a2,.., aN}
            See which action can obtain the largest Q value

        Solution 2
            
            Using gradient ascent to solve the optimization problem.

        Solution 3
            
            Design a network to make the optimization easy.

                           NN          
                          |--| ->  Î¼(s) vector
                    s ->  |QÏ€| ->  Î£(s) matrix
                          |--| ->  V(s) Sclar

                Q(s, a)=-(a - Î¼(s))^T Î£(s) (a - Î¼(s)) + V(s)
                Î¼(s) = arg max Q(s, a)
                            a

        Solution 4 
            Don't use Q-learning
                Policy-based    Learning a actor
                value-based     Learning a critic

                Policy-based + value-based      Actor + Critic


Asynchronous Advantage Actor-Critic (A3C)

                                  N  T   T                  
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£ ( Î£ Î³^(t' - t)r -b) âˆ‡logP(at|st, Î¸)           b å¯ä»¥å– E[R] ä¹Ÿå¯ä»¥æ˜¯state-dependent
                                 i=1t=1 t'=t              
                                        ------------------
                                        Advantage Function AÎ¸(st, at)
                                        How good it is if we take afother than other actions at st.
                                        Estimated by "critic" (later)

                                  N  T                      
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£ AÎ¸(st, at) âˆ‡logP(at|st, Î¸)           
                                 i=1t=1                   

            ç›´è§‰å°±æ˜¯: åœ¨Ï„i é‡‡å– at åœ¨st çŠ¶æ€ä¸‹ R(Ï„) æ˜¯æ­£çš„ï¼Œæ”¹å˜Î¸ å˜å¤§ P(at|st)
                                                      è´Ÿçš„ï¼Œæ”¹å˜Î¸ å˜å° P(at|st)

            æ³¨æ„è¿™é‡ŒR(Ï„) æ˜¯ä¸€ä¸ªepisodeçš„total reward

            åŠ logï¼Œå¾®åˆ†ä¹‹åé™¤æ‰ä¸€ä¸ªå‡ ç‡ åšä¸€ä¸ªæ­£åˆ™åŒ–
                                                               ^
           åœ¨classificationä¸­ æˆ‘ä»¬minimize corss enptory  : -Î£ yi logyi

                                  ç­‰ä»·äº maximize logyi  å°±æ˜¯ ä¸Šé¢ logP(at|st, Î¸) åœ¨å½“å‰çŠ¶æ€é‡‡å–æŸåŠ¨ä½œçš„æ¦‚ç‡ å¯ä»¥çœ‹æˆåˆ†ç±»é—®é¢˜(é‡‡å–æŸä¸ªåŠ¨ä½œ)

                                  ä½†æˆ‘ä»¬è¿˜ä¹˜ä¸Šäº† R(Ï„) å¯ä»¥çœ‹æˆ classification x å¯¹åº”çš„æ¬¡æ•° ï¼ˆ1 = 1æ¬¡ï¼‰ ä¹Ÿå¯ä»¥è½¬æ¢ä¸ºåˆ†ç±»é—®é¢˜
                                        æ˜¯ä¸æ˜¯ä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯å¢åŠ å¥–åŠ±å¤§çš„çŠ¶æ€ä¸‹é‡‡å–çš„åŠ¨ä½œ

            æ„Ÿæ‚Ÿï¼šNN(actor)çš„å‚æ•°ä½œä¸ºç­–ç•¥ï¼Œè¾“å…¥æ˜¯çŠ¶æ€ï¼Œè¾“å‡ºçš„åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
                  çŸ¥é“æ¢¯åº¦æ›´æ–°çš„å…¬å¼äº†
                                              N  T                    
                        âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£  R(Ï„)âˆ‡logP(at|st, Î¸) 
                                             i=1t=1                   
                  å…ˆç”¨actoræ”¶é›†ä¸€å¤§å †(s, a), å’Œæ¯ä¸ªtrajectory çš„ total Reward
                  ä»£å…¥æ¢¯åº¦æ›´æ–°å…¬å¼ æŠŠgradient ç®—å‡ºæ¥
                    å¢åŠ å¥–åŠ±å¤§çš„çŠ¶æ€ä¸‹çš„é‡‡å–çš„åŠ¨ä½œçš„æ¦‚ç‡
                  æ›´æ–°Î¸

                  å°±æ˜¯æ”¶é›†æ•°æ® -> æ›´æ–°æ¨¡å‹ å¾ªç¯
                  
                  å¯ä»¥æ˜¯classification ä½†lossfunctionè¦ä¹˜è´Ÿå·å’Œtotal reward

            å¦‚æœç™¼ç¾ä¸èƒ½å¾®åˆ†å°±ç”¨ policy gradient ç¡¬ train ä¸€å‘  -- ç¯å¢ƒå’Œrewardæ˜¯é»‘ç›’å­


             T            
             Î£ Î³^(t' - t)r
            t'=t          
            --------------
            Gt: obtained via interaction    ä»å½“å‰åˆ°å›åˆç»“æŸè·å¾—çš„reward
                unstable
                
                äº’åŠ¨processæœ‰éšæœºæ€§ total reward ä¹Ÿæ˜¯éšæœºçš„ 
                å¯èƒ½æœ‰ä¸€ä¸ªdistribution, ä½†æˆ‘ä»¬æ˜¯åšå°‘é‡sample
            
                
                With sufficient samples, approximate the expectation of G.
            
                        -> G = 100
                        -> G = 3
                 s -> a -> G = 1
                        -> G = 2
                        -> G = -10

                 Can we estimate the expected value of G?   --  è®©training æ›´ç¨³å®š
                        using a critic

                        â€¢ State value function VÏ€(s)
                            â€¢ When using actor Ï€, the cumulated reward expects to be obtained after visiting state s

                        â€¢ State-action value function QÏ€(s, a)
                            â€¢ When using actor Ï€, the cumulated reward expects to be obtained after taking a at state s
                            output:
                                the cumulated reward expects to be obtained after seeing observation (state) s and taking action a 
                                QÏ€(s, a)
                                or
                                the cumulated reward expects to be obtained after seeing observation (state) s and taking action a(actionç©ºé—´å¾ˆå°çš„æƒ…å†µä¸‹ï¼Œå±•å¼€æ‰€æœ‰actionï¼Œè¿™æ ·è¾“å…¥çš„æ—¶å€™å°±ä¸éœ€è¦actionäº†)
                                QÏ€(s, a = left) QÏ€(s, a = right) QÏ€(s, a = fire)

                                           ----
                                      s -> |  |
                                           |QÏ€| -> QÏ€(s, a)
                                      a -> |  |
                                           ----
                                
                                           ----
                                           |  | -> QÏ€(s, a=left)
                                      s -> |QÏ€| -> QÏ€(s, a=right)       for discrete action only
                                           |  | -> QÏ€(s, a=fire)
                                           ----

                        Estimated by TD or MC

                                  N  T   T                  
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£ ( Î£ Î³^(t' - t)r -b) âˆ‡logP(at|st, Î¸)           b å¯ä»¥å– E[R] ä¹Ÿå¯ä»¥æ˜¯state-dependent
                                 i=1t=1 t'=t              
                                        --------------
                                        Gt: obtained via interaction    ä»å½“å‰åˆ°å›åˆç»“æŸè·å¾—çš„reward
                                        Estimated by "critic" (later)

                                        E[Gt] = QÏ€Î¸(st, at)  æœ‰involve action

                                        b -- baseline VÏ€Î¸(st) æ²¡æœ‰involve action


                                  N  T        
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£ (QÏ€(st, at) - VÏ€(st)) âˆ‡logP(at|st, Î¸)
                                 i=1t=1     
                        Estimate two networks? We can only estimate one.

            QÏ€(st, at) = E[rt + VÏ€(st+1)]
                        rt random variable
                        æŠŠæœŸæœ›å»æ‰  paper é‡Œé¢æ˜¯è¿™æ ·
                       = rt + VÏ€(st+1)

                                  N  T        
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£ (rt + VÏ€(st+1) - VÏ€(st)) âˆ‡logP(at|st, Î¸)
                                 i=1t=1     
                        Only estimate state value


            
    loop:
        Ï€ interacts with the environment

                |               1.è’™ç‰¹å¡æ´›                    
                v               2.Temporal-Difference (TD) 

        Learning VÏ€(s,a)        

                |               
                v               

        Update actor from Ï€ â†’ Ï€'(Actorç½‘ç»œå‚æ•°) based on VÏ€(s)
            Actorç½‘ç»œåšGradient Ascentï¼Œæ¨å¯¼çœ‹ä¸Šé¢
                                  N  T        
            âˆ‡E[R_{Î¸}] è¿‘ä¼¼= 1/N * Î£  Î£ (rt + VÏ€(st+1) - VÏ€(st)) âˆ‡logP(at|st, Î¸)
                                 i=1t=1     
                        Only estimate state value
                                        -----------------------
                                        Advantage function

        â€¢ Tips
            â€¢ The parameters of actor Ï€(s) and critic VÏ€(s) can be shared

                                                  NN
                                                |-----|
                                NN          ->  |Actor| -> action probability distribution
                          |--------------|      |-----|
                    s ->  |shared network| 
                          |--------------|  ->  |------|
                                                |Critic| -> VÏ€(s)
                                                |------|
                                                   NN

                    shared network can be CNN : image pixel -> high level information
            
            â€¢ Use output entropy as regularization for Ï€ (s) 
                â€¢ Larger entropy is preferred â†’ exploration


   Asynchronous Advantage Actor-Critic (A3C) 


                                           Global Network

                                        NN               NN                      
                                      |-----|         |------|                          
                                      |Actor|         |Critic|                                                             
                                      |-----|         |------|                          
                                                                                       
                                                NN                                          
                                          |--------------|                                           
                                          |shared network|                                  
                                          |--------------|                               
                                                 ^              
                                                 |              
                                              input(s)          
                                                                                                    


    
                            worker1                             worker2                 ...         workern
                      NN               NN                 NN               NN                                                                                       
                    |-----|         |------|            |-----|         |------|                                                                                    
                    |Actor|         |Critic|            |Actor|         |Critic|                                                                                    
                    |-----|         |------|            |-----|         |------|                                                                                    
                                                                                                                                                                    
                              NN                                  NN                                                                                                
                        |--------------|                    |--------------|                                                                                        
                        |shared network|                    |shared network|                                                                                        
                        |--------------|                    |--------------|                                                                                        
                               ^                                   ^                                                                                                
                               |                                   |                                                                                                
                            input(s)                            input(s)                                                                                            
                                                                                                                    
                                                                                                                    
                            Env1                                  Env2                  ....           Envn                                                                                                   
                                                                                                    
                                                                                                    
        1.worker_i Copy global parameters                                                                                                        
        2.worker_i Sampling some data
        3.worker_i compute gradients
        4.update global models  (other workers also update models)


Pathwise Derivative Policy Gradient
    å¯ä»¥çœ‹æˆQ-learning è§£ Continuous action çš„ä¸€ç§æ–¹æ³•
    ä¹Ÿå¯ä»¥çœ‹æˆä¸€ç§ç‰¹åˆ«çš„Actor-Criticæ–¹æ³•
        åŸæ¥çš„Actor-Criticæ–¹æ³•ï¼šå½“å‰è¡Œä¸ºå¥½è¿˜æ˜¯ä¸å¥½
        Pathwise derivative policy gradientï¼šå‘Šè¯‰actor é‡‡å–ä»€ä¹ˆæ ·çš„æ–¹æ³•æ˜¯å¥½çš„
        
    Action a is a continuous Vector
        
        a = arg max Q(s, a)
                 a
        å¦‚ä½•è§£è¿™ä¸ªå…¬å¼ï¼šç”¨ä¸€ä¸ªNNï¼Œs -> NN(Actor) -> a æ¥è¡¨ç¤ºè¿™ä¸ªå…¬å¼

        Actor as the solver of this optimization problem 

        

        Ï€'(s) = arg max QÏ€(s, a)    a is the output of an actor 
                     a 

                           s -> |---|
                  NN            |   |
                |-----|         |QÏ€ | -> QÏ€(s, a)
          s ->  |Actor| -> a -> |   |
                |-----|         |---|

                -----------------------
                This is a large network

            Fixed QÏ€ å»è°ƒ Actorçš„å‚æ•°

    loop:
        Ï€ interacts with the environment

                |               1.è’™ç‰¹å¡æ´›                    
                v               2.Temporal-Difference (TD) 

        Learning QÏ€(s,a)

                |               "Better":VÏ€'(st) >= VÏ€(st) for all state s
                v               Ï€'(s) = arg maxQÏ€(s, a)
                                             a
                                problem:Not suitable for continuous action a (solve it later) å› ä¸ºè¦ç©·ä¸¾æ‰€æœ‰çš„action
                                A solution : Pathwise Derivative Policy Gradient å¢åŠ ä¸€ä¸ªç½‘ç»œ(actor)äº§ç”Ÿè¿ç»­çš„åŠ¨ä½œ (DDPG)

                                                   ----
                                              s -> |  |
                                    |-----|        |QÏ€| -> QÏ€(s, a)
                                s ->|Actor|-> a -> |  |
                                    | Ï€   |        ----
                                    -------

        Find a new actor Ï€' "better" than Ï€    Ï€'=Ï€
        
        è¿™é‡Œä¹Ÿå¯ä»¥ç”¨Reply buffer å’Œ exploration


        Typical Q-Learning Algorithm                                                                Pathwise Derivative Policy Gradient                                                  
                                                                                                                                                                                         
                                                         ^                                                                                           ^                            ^                       
            â€¢ Initialize Q-function Q, target Q-function Q = Q                                          â€¢ Initialize Q-function Q, target Q-function Q = Q, actor Ï€, target actor Ï€ = Ï€                       
            â€¢ In each episode                                                                           â€¢ In each episode                                                                
                â€¢ For each time step t                                                                      â€¢ For each time step t                                                       
                    â€¢ Given state st, take action a, based on Q (epsilon greedy)                            â€¢ Given state st, take action a, based on Ï€  (epsilon greedy) -- first change                                         
                    â€¢ Obtain reward rt, and reach new state st+1                                            â€¢ Obtain reward rt, and reach new state st+1                                                          
                    â€¢ Store (st, at, rt, st+1) into butter                                                  â€¢ Store (st, at, rt, st+1) into butter                                                        
                    â€¢ Sample (si, ai, ri, si+1) from buffer (usually a batch)                               â€¢ Sample (si, ai, ri, si+1) from buffer (usually a batch)                                                         
                                          ^                                                                                   ^       ^                                                  
                    â€¢ Target y = ri + max Q(si+1, a)                                                        â€¢ Target y = ri + Q(si+1, Ï€(si+1)) -- second change                                       
                    â€¢ Update the parameters of Q to make Q(si, ai) close to y (regression)                  â€¢ Update the parameters of Ï€ to maximize Q(si, Ï€(si)) -- third change                                                         
                                          ^                                                                                       ^                                                     
                    â€¢ Every C steps reset Q = Q                                                             â€¢ Every C steps reset Q = Q                                                           
                                                                                                                                  ^                                                     
                                                                                                            â€¢ Every C steps reset Ï€ = Ï€  -- fourth change                                               
Connection with GAN

    Method                          GANS        AC
    Freezing learning               yes         yes
    Label smoothing                 yes         no 
    Historical averaging            yes         no 
    Minibatch discrimination        yes         no
    Batch normalization             yes         yes
    Target networks                 n/a         yes
    Replay buffers                  no          yes
    Entropy regularization          no          yes
    Compatibility                   no          yes
